[
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "CORS",
        "importPath": "flask_cors",
        "description": "flask_cors",
        "isExtraImport": true,
        "detail": "flask_cors",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urljoin",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "RobotFileParser",
        "importPath": "urllib.robotparser",
        "description": "urllib.robotparser",
        "isExtraImport": true,
        "detail": "urllib.robotparser",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "clean_html_for_json",
        "kind": 2,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "def clean_html_for_json(soup):\n    \"\"\"\n    Removes non-content elements to isolate the main body text.\n    \"\"\"\n    for element in soup([\"script\", \"style\", \"head\", \"noscript\", \"meta\", \"[document]\"]):\n        element.extract()\n    for selector in ['header', 'nav', 'footer', '.sidebar', '#menu', '.ad-unit']:\n        for element in soup.select(selector):\n            element.extract()\n    text = soup.get_text(separator=' ', strip=True)",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "is_scraping_allowed",
        "kind": 2,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "def is_scraping_allowed(url, user_agent):\n    \"\"\"Checks the website's robots.txt file.\"\"\"\n    try:\n        parsed_url = urlparse(url)\n        robots_url = urljoin(f\"{parsed_url.scheme}://{parsed_url.netloc}\", \"robots.txt\")\n        rp = RobotFileParser()\n        rp.set_url(robots_url)\n        rp.read()\n        return rp.can_fetch(user_agent, url)\n    except Exception:",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "scrape_url_for_api",
        "kind": 2,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "def scrape_url_for_api(url):\n    \"\"\"Fetches HTML with retry logic, cleans text, and safely extracts the title.\"\"\"\n    if not url.startswith('http://') and not url.startswith('https://'):\n        url = \"https://\" + url \n    if not is_scraping_allowed(url, USER_AGENT):\n        return {\n            \"title\": \"ROBOTS_DISALLOWED\",\n            \"text\": \"Scraping was explicitly disallowed by robots.txt.\",\n        }\n    for attempt in range(MAX_RETRIES):",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "scrape_single_data",
        "kind": 2,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "def scrape_single_data():\n    # Expects a single company object from the React component\n    data = request.json \n    if data is None or not isinstance(data, dict):\n        return jsonify({\"error\": \"Invalid JSON body. Expected a single company object.\"}), 400\n    company_object = data\n    url = company_object.get('website')\n    if not url:\n        scraped_data = {\"title\": \"MISSING_URL\", \"text\": \"Website URL was missing.\"}\n    else:",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "scrape_data",
        "kind": 2,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "def scrape_data():\n    data = request.json\n    if data is None or not isinstance(data, list):\n        return jsonify({\"error\": \"Invalid JSON body or missing content type header. Expected a list.\"}), 400\n    company_data_list = data\n    scraped_results = []\n    for company_object in company_data_list:\n        url = company_object.get('website')\n        # Ensure scraped_data is assigned in all logical branches\n        if not url:",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "USER_AGENT",
        "kind": 5,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "USER_AGENT = \"WebPageAnalyzer/1.0 (Contact: user@example.com) Python-Requests\"\nMAX_RETRIES = 1\nBASE_BACKOFF_TIME = 20 # Initial wait in seconds for exponential backoff\napp = Flask(__name__)\nCORS(app) \n# --- Helper Functions ---\ndef clean_html_for_json(soup):\n    \"\"\"\n    Removes non-content elements to isolate the main body text.\n    \"\"\"",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "MAX_RETRIES",
        "kind": 5,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "MAX_RETRIES = 1\nBASE_BACKOFF_TIME = 20 # Initial wait in seconds for exponential backoff\napp = Flask(__name__)\nCORS(app) \n# --- Helper Functions ---\ndef clean_html_for_json(soup):\n    \"\"\"\n    Removes non-content elements to isolate the main body text.\n    \"\"\"\n    for element in soup([\"script\", \"style\", \"head\", \"noscript\", \"meta\", \"[document]\"]):",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "BASE_BACKOFF_TIME",
        "kind": 5,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "BASE_BACKOFF_TIME = 20 # Initial wait in seconds for exponential backoff\napp = Flask(__name__)\nCORS(app) \n# --- Helper Functions ---\ndef clean_html_for_json(soup):\n    \"\"\"\n    Removes non-content elements to isolate the main body text.\n    \"\"\"\n    for element in soup([\"script\", \"style\", \"head\", \"noscript\", \"meta\", \"[document]\"]):\n        element.extract()",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "app = Flask(__name__)\nCORS(app) \n# --- Helper Functions ---\ndef clean_html_for_json(soup):\n    \"\"\"\n    Removes non-content elements to isolate the main body text.\n    \"\"\"\n    for element in soup([\"script\", \"style\", \"head\", \"noscript\", \"meta\", \"[document]\"]):\n        element.extract()\n    for selector in ['header', 'nav', 'footer', '.sidebar', '#menu', '.ad-unit']:",
        "detail": "backend.geo_scraper",
        "documentation": {}
    }
]