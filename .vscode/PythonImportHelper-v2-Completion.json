[
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "CORS",
        "importPath": "flask_cors",
        "description": "flask_cors",
        "isExtraImport": true,
        "detail": "flask_cors",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urljoin",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "RobotFileParser",
        "importPath": "urllib.robotparser",
        "description": "urllib.robotparser",
        "isExtraImport": true,
        "detail": "urllib.robotparser",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "clean_html_for_json",
        "kind": 2,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "def clean_html_for_json(soup):\n    \"\"\"\n    Removes non-content elements to isolate the main body text.\n    \"\"\"\n    for element in soup([\"script\", \"style\", \"head\", \"noscript\", \"meta\", \"[document]\"]):\n        element.extract()\n    for selector in ['header', 'nav', 'footer', '.sidebar', '#menu', '.ad-unit']:\n        for element in soup.select(selector):\n            element.extract()\n    text = soup.get_text(separator=' ', strip=True)",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "is_scraping_allowed",
        "kind": 2,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "def is_scraping_allowed(url, user_agent):\n    \"\"\"Checks the website's robots.txt file.\"\"\"\n    try:\n        parsed_url = urlparse(url)\n        robots_url = urljoin(f\"{parsed_url.scheme}://{parsed_url.netloc}\", \"robots.txt\")\n        rp = RobotFileParser()\n        rp.set_url(robots_url)\n        rp.read()\n        return rp.can_fetch(user_agent, url)\n    except Exception:",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "scrape_url_for_api",
        "kind": 2,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "def scrape_url_for_api(url):\n    \"\"\"Fetches HTML with retry logic, cleans text, and safely extracts the title.\"\"\"\n    if not url.startswith('http://') and not url.startswith('https://'):\n        url = \"https://\" + url \n    if not is_scraping_allowed(url, USER_AGENT):\n        return {\n            \"title\": \"ROBOTS_DISALLOWED\",\n            \"text\": \"Scraping was explicitly disallowed by robots.txt.\",\n        }\n    for attempt in range(MAX_RETRIES):",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "scrape_data",
        "kind": 2,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "def scrape_data():\n    data = request.json\n    if data is None or not isinstance(data, list):\n        return jsonify({\"error\": \"Invalid JSON body or missing content type header. Expected a list.\"}), 400\n    company_data_list = data\n    scraped_results = []\n    for company_object in company_data_list:\n        url = company_object.get('website')\n        # Ensure scraped_data is assigned in all logical branches\n        if not url:",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "USER_AGENT",
        "kind": 5,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "USER_AGENT = \"WebPageAnalyzer/1.0 (Contact: user@example.com) Python-Requests\"\nMAX_RETRIES = 1\nBASE_BACKOFF_TIME = 20 # Initial wait in seconds for exponential backoff\napp = Flask(__name__)\nCORS(app) \n# --- Helper Functions ---\ndef clean_html_for_json(soup):\n    \"\"\"\n    Removes non-content elements to isolate the main body text.\n    \"\"\"",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "MAX_RETRIES",
        "kind": 5,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "MAX_RETRIES = 1\nBASE_BACKOFF_TIME = 20 # Initial wait in seconds for exponential backoff\napp = Flask(__name__)\nCORS(app) \n# --- Helper Functions ---\ndef clean_html_for_json(soup):\n    \"\"\"\n    Removes non-content elements to isolate the main body text.\n    \"\"\"\n    for element in soup([\"script\", \"style\", \"head\", \"noscript\", \"meta\", \"[document]\"]):",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "BASE_BACKOFF_TIME",
        "kind": 5,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "BASE_BACKOFF_TIME = 20 # Initial wait in seconds for exponential backoff\napp = Flask(__name__)\nCORS(app) \n# --- Helper Functions ---\ndef clean_html_for_json(soup):\n    \"\"\"\n    Removes non-content elements to isolate the main body text.\n    \"\"\"\n    for element in soup([\"script\", \"style\", \"head\", \"noscript\", \"meta\", \"[document]\"]):\n        element.extract()",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "backend.geo_scraper",
        "description": "backend.geo_scraper",
        "peekOfCode": "app = Flask(__name__)\nCORS(app) \n# --- Helper Functions ---\ndef clean_html_for_json(soup):\n    \"\"\"\n    Removes non-content elements to isolate the main body text.\n    \"\"\"\n    for element in soup([\"script\", \"style\", \"head\", \"noscript\", \"meta\", \"[document]\"]):\n        element.extract()\n    for selector in ['header', 'nav', 'footer', '.sidebar', '#menu', '.ad-unit']:",
        "detail": "backend.geo_scraper",
        "documentation": {}
    },
    {
        "label": "_Known",
        "kind": 6,
        "importPath": "node_modules.flatted.python.flatted",
        "description": "node_modules.flatted.python.flatted",
        "peekOfCode": "class _Known:\n    def __init__(self):\n        self.key = []\n        self.value = []\nclass _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0",
        "detail": "node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_String",
        "kind": 6,
        "importPath": "node_modules.flatted.python.flatted",
        "description": "node_modules.flatted.python.flatted",
        "peekOfCode": "class _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0\n    for _ in value:\n        keys.append(i)\n        i += 1\n    return keys",
        "detail": "node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "node_modules.flatted.python.flatted",
        "description": "node_modules.flatted.python.flatted",
        "peekOfCode": "def parse(value, *args, **kwargs):\n    json = _json.loads(value, *args, **kwargs)\n    wrapped = []\n    for value in json:\n        wrapped.append(_wrap(value))\n    input = []\n    for value in wrapped:\n        if isinstance(value, _String):\n            input.append(value.value)\n        else:",
        "detail": "node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "node_modules.flatted.python.flatted",
        "description": "node_modules.flatted.python.flatted",
        "peekOfCode": "def stringify(value, *args, **kwargs):\n    known = _Known()\n    input = []\n    output = []\n    i = int(_index(known, input, value))\n    while i < len(input):\n        output.append(_transform(known, input, input[i]))\n        i += 1\n    return _json.dumps(output, *args, **kwargs)",
        "detail": "node_modules.flatted.python.flatted",
        "documentation": {}
    }
]